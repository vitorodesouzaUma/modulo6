{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef3c355-64a9-4167-8387-5318d8bf3745",
   "metadata": {},
   "source": [
    "# **Module 6: Descriptive and Predictive Modeling**\n",
    "## **Exercise 1:** K-Nearest Neighbors in depth\n",
    "### **Submitted by:** Vitor Oliveira de Souza (Z0963220P), Jorge De La Torre (DNI), Phoebe (DNI), Miguel Galán (DNI)\n",
    "### **Date:** 13/02/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea586e-6b78-4747-8ff9-e01f479e95ca",
   "metadata": {},
   "source": [
    "#### **A)** \"from sklearn.datasets import load_breast_cancer\" (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2c49e9-679d-4519-be0d-49379409b14b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:36.136220Z",
     "start_time": "2024-02-17T11:33:35.833649Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184431ab-4ca7-466f-b7cf-9bc85f461c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:36.153537Z",
     "start_time": "2024-02-17T11:33:36.136799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset from sklearn datasets\n",
    "dataset = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5046356-52bb-4a74-9c4e-0c48e7d710f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:36.188042Z",
     "start_time": "2024-02-17T11:33:36.157562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get input data from dataset\n",
    "X = pd.DataFrame(dataset.data)\n",
    "# Get feature names\n",
    "X.columns = dataset.feature_names\n",
    "# Get output labels\n",
    "y = pd.DataFrame(dataset.target)\n",
    "y.columns = ['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66795a7-6baa-490b-aaed-b102a938cc84",
   "metadata": {},
   "source": [
    "#### 1) Describe the dataset in dimensions such as number of features, number of categories, and number of samples per category using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af42f87e-e5d4-4bf1-95fd-70fca4f3e01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:36.246281Z",
     "start_time": "2024-02-17T11:33:36.168724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look in input data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3360e97-f569-4385-96d9-005ea5c0e5f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:36.248969Z",
     "start_time": "2024-02-17T11:33:36.201315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse input and output data shapes\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb520d4-dc6a-4ff0-aaf5-b68725e510ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:36.266615Z",
     "start_time": "2024-02-17T11:33:36.207453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "benign       357\n",
       "malignant    212\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get labels names\n",
    "target_names = dataset.target_names\n",
    "y['labels'] = y['target'].map(lambda x: target_names[x])\n",
    "y.groupby('labels').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c787d53-7103-40c7-b708-f4c078ee48e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:36.416950Z",
     "start_time": "2024-02-17T11:33:36.222417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst radius  \\\n",
       "count     569.000000              569.000000  ...    569.000000   \n",
       "mean        0.181162                0.062798  ...     16.269190   \n",
       "std         0.027414                0.007060  ...      4.833242   \n",
       "min         0.106000                0.049960  ...      7.930000   \n",
       "25%         0.161900                0.057700  ...     13.010000   \n",
       "50%         0.179200                0.061540  ...     14.970000   \n",
       "75%         0.195700                0.066120  ...     18.790000   \n",
       "max         0.304000                0.097440  ...     36.040000   \n",
       "\n",
       "       worst texture  worst perimeter   worst area  worst smoothness  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       worst compactness  worst concavity  worst concave points  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       worst symmetry  worst fractal dimension  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse input data distribution\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f50963b-4591-4410-a788-8017572ed04a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA.fit_transform() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrincipal Component 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     73\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrincipal Component 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[43mprint_pca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m, in \u001b[0;36mprint_pca\u001b[1;34m(data, y, highlight_index, highlight_points, text, dim_reducer, n_components, scaler)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Perform PCA\u001b[39;00m\n\u001b[0;32m     22\u001b[0m pca \u001b[38;5;241m=\u001b[39m dim_reducer(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m---> 23\u001b[0m X_pca \u001b[38;5;241m=\u001b[39m \u001b[43mdim_reducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m X_pca \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_pca, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPC1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPC2\u001b[39m\u001b[38;5;124m'\u001b[39m], index\u001b[38;5;241m=\u001b[39mX_scaled\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     25\u001b[0m X_pca[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmalign\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m text\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenign\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mTypeError\u001b[0m: PCA.fit_transform() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "# Visualise the data in 2D using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Create a function because we'll use it later to check the model's results\n",
    "def print_pca(data, \n",
    "              y, \n",
    "              highlight_index=None,\n",
    "              highlight_points=None,\n",
    "              text='',\n",
    "              dim_reducer=PCA(n_components=2),\n",
    "              n_components = 2,\n",
    "              scaler=MinMaxScaler()\n",
    "             ):\n",
    "    \n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "    \n",
    "    # Perform PCA\n",
    "    X_pca = dim_reducer.fit_transform(X_scaled)\n",
    "    X_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=X_scaled.index)\n",
    "    X_pca['labels'] = [text+'malign' if target == 0 else text+'benign' for target in y['target']]\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    scatter = ax.scatter(x=X_pca['PC1'],\n",
    "               y=X_pca['PC2'],\n",
    "               c=y['target'],\n",
    "               cmap='bwr',\n",
    "               s=60,alpha=0.3)\n",
    "\n",
    "    # Plot highlighted points based on the index list provided\n",
    "    if highlight_index is not None:\n",
    "\n",
    "        print(X_pca.loc[highlight_index].index)\n",
    "        \n",
    "        highlighted = ax.scatter(x=X_pca['PC1'].loc[highlight_index],\n",
    "                                 y=X_pca['PC2'].loc[highlight_index],\n",
    "                                 c='lime', marker='x', s=60, alpha=1)\n",
    "        # Annotate highlighted points\n",
    "        for i in highlight_index:\n",
    "            ax.annotate(X_pca['labels'].loc[i], \n",
    "                        (X_pca['PC1'].loc[i], X_pca['PC2'].loc[i]),\n",
    "                        textcoords=\"offset points\", # Positioning of the text\n",
    "                        xytext=(0,10), # Distance from the point to the text\n",
    "                        ha='center') # Horizontal alignment\n",
    "    \n",
    "    # Highlight additional points specified directly that are not in the main dataset\n",
    "    if highlight_points is not None:\n",
    "        \n",
    "        # Scale the highlight_points with the same scaled\n",
    "        highlight_points_scaled = pd.DataFrame(scaler.transform(highlight_points), columns=highlight_points.columns, index=highlight_points.index)\n",
    "        # Transform using PCA using the same pca transformer\n",
    "        highlight_points_pca = dim_reducer.transform(highlight_points_scaled)\n",
    "        \n",
    "        # Plot the highlighted points\n",
    "        for point in highlight_points_pca:\n",
    "            ax.scatter(point[0], point[1], c='gold', edgecolor='black', marker='o', s=60, alpha=1)\n",
    "\n",
    "    \n",
    "    # Create custom legend\n",
    "    classes = dataset.target_names\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=classes[i],\n",
    "                              markerfacecolor='b' if i==0 else 'r', markersize=10) for i in range(len(classes))]\n",
    "    ax.legend(handles=legend_elements, loc=\"lower left\", title=\"Classes\")\n",
    "        \n",
    "    # Set titles and labels\n",
    "    ax.set_title('PCA of Dataset')\n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "    ax.set_ylabel('Principal Component 2')\n",
    "\n",
    "print_pca(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbf900e-3fa0-49ef-89ca-c58d71cf4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pca_3d(data,y):\n",
    "    # Now visualize the data in 3d\n",
    "    %matplotlib qt\n",
    "\n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    X_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2','PC3'])\n",
    "    X_pca['label'] = ['malignant' if target == 0 else 'benign' for target in y['target']]\n",
    "    \n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_pca['PC1'],\n",
    "               X_pca['PC2'],\n",
    "               X_pca['PC3'],\n",
    "               c=y['target'],\n",
    "               cmap='bwr',\n",
    "               s=60,alpha=0.5)\n",
    "\n",
    "print_pca_3d(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf32a63-2549-4f69-a210-5a83edd06c76",
   "metadata": {},
   "source": [
    "#### Answer 1:\n",
    "##### **Number of features**: The dataset comprises 30 features. These features might include various measurements or characteristics relevant to the study of breast cancer.\n",
    "##### **Number of categories**: There are two categories within the dataset: Malignant and Benign. These categories represent the classification targets for our analysis, indicating whether samples are indicative of malignant (cancerous) or benign (non-cancerous) conditions.\n",
    "##### **Number of samples in each category**: The dataset contains 212 samples classified as malignant. These represent cases where the condition is potentially harmful and requires detailed examination. Conversely, there are 357 samples identified as benign, indicating conditions that are generally not harmful.\n",
    "##### **Inbalanced data**: An initial observation suggests that the dataset is somewhat imbalanced, with a greater number of benign samples compared to malignant ones. This imbalance could influence the performance of machine learning models, necessitating techniques such as weighted evaluation metrics to ensure model reliability across both categories.\n",
    "##### **Presence of mixed datapoints/outliers**: Visual inspection, particularly through 2D and 3D projections with PCA, reveals that there is some degree of overlap between the categories. Points that is labeled with malign category are found within clusters of the benign category, indicating the potential presence of **outliers** or **challenging cases**.\n",
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efd0b5-4ee5-41e1-8930-32a6195dcc43",
   "metadata": {},
   "source": [
    "#### 2) Represent the statistical support of every feature graphically, resorting to Matplotlib’s boxplot function. Are there any outliers that can be detected by simple visual inspection? If so, devise a handcrafted method to detect and isolate such examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4f33e-9912-4f3a-a555-321ebbedb2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:37.080554Z",
     "start_time": "2024-02-17T11:33:36.308066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check all the boxplots together\n",
    "%matplotlib inline\n",
    "scaler = StandardScaler()\n",
    "X_standard_scaled = pd.DataFrame(scaler.fit_transform(X))\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.boxplot(X_standard_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d62eb8-40ed-4a4e-9ab5-bf8b97349c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Altough it's a widely distribuited data, it seems there're some points too far away from the median value\n",
    "# Let's clean the outliers considering 5 times the IQR interval as a threashold\n",
    "def get_outliers_indices(data, columns_outliers):\n",
    "    indices_to_remove = []\n",
    "    for column in columns_outliers:\n",
    "        q1 = data[column].quantile(0.25)\n",
    "        q3 = data[column].quantile(0.75)\n",
    "        IQR = q3 - q1\n",
    "        # Find the indices outside the acceptable range\n",
    "        outliers = data[(data[column] < q1 - 5 * IQR) | (data[column] > q3 + 5 * IQR)].index\n",
    "        indices_to_remove.extend(outliers)  # Use extend to flatten the list\n",
    "    indices_to_remove = list(set(indices_to_remove))  # Remove duplicates\n",
    "    return indices_to_remove\n",
    "    \n",
    "# Lets join X and y to clean them at the same time\n",
    "df = X.join(y)\n",
    "columns_outliers = [col for col in df.columns if col not in ['target', 'labels']]\n",
    "outliers_index = get_outliers_indices(df,columns_outliers)\n",
    "outliers_index\n",
    "df_cleaned = df.drop(outliers_index)\n",
    "df_cleaned.groupby('target').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfcceab-e2b3-4b7b-9757-5fcfc661dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the outliers that were removed\n",
    "print_pca(X,y,outliers_index,text='outlier: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a8474-05e3-4822-819d-5a5ddaeb5f0a",
   "metadata": {},
   "source": [
    "#### Answer 2:\n",
    "##### The boxplot visualization indicates a wide distribution across almost all features, with numerous data points lying outside the whiskers. These points are typically considered outliers in a conventional analysis context. However, our assessment suggests that these aren't outliers in the traditional sense but rather indicate the presence of subgroups within the data, each with distinct distributions.\n",
    "##### This heterogeneity within the data is not necessarily detrimental; on the contrary, it could enrich our analysis by revealing underlying patterns that assist in classifying the dataset accurately. It highlights the complexity of the data and suggests that multiple factors may influence the outcomes we are trying to predict.\n",
    "\n",
    "### Approach to Outlier Detection:\n",
    "##### Despite recognizing the value of the wide data distribution, we identified certain points as extreme outliers, significantly distanced from the majority of data points in specific features. These extreme values could potentially skew our analysis and model training.\n",
    "##### To address outliers, we applied a method focusing on the interquartile range (IQR). Specifically, we set a threshold at 5 times the IQR from the first and third quartiles. This approach aims to retain the inherent data structure and variability while excluding extreme outliers that could adversely affect our analysis.\n",
    "##### We implemented a Python function to calculate the IQR for each feature and identify indices of data points lying beyond the acceptable range (5 times the IQR from the Q1 and Q3 quartiles). These indices represent the outliers we decided to remove.\n",
    "##### **Important:** The threashold of 5 times IQR was set by visual inpecting of the features distributions.\n",
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e58794-c4bd-4df4-b7cd-15ac61a5c2d5",
   "metadata": {},
   "source": [
    "#### 3) Repeat each of the experiments seen in class with the K-Nearest Neighbors model, providing arguments for each of the steps taken along the process, and commenting on the partial results obtained with the selected dataset. Please use as many performance metrics as needed to illustrate the particularities of the selected dataset (e.g. imbalanced classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f083d66-f552-4036-b5a2-890b84319365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:47.452652Z",
     "start_time": "2024-02-17T11:33:47.424013Z"
    }
   },
   "outputs": [],
   "source": [
    "# First we separate the input and output again (after outlier cleaning)\n",
    "X_cleaned = df_cleaned.drop(columns=['target','labels'])\n",
    "y_cleaned = df_cleaned[['target','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25872f-424c-4ed4-9ca5-1c2ee540aa6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:47.529941Z",
     "start_time": "2024-02-17T11:33:47.431443Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Now we create a splitter to split the data considering the classes\n",
    "# If we simply split the data randomly, the unbalanced classes could be splitted unevenly in train and test\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in sss.split(X_cleaned, y_cleaned):\n",
    "    Xtrain, ytrain = X_cleaned.iloc[train_index], y_cleaned.iloc[train_index]\n",
    "    Xtest, ytest = X_cleaned.iloc[test_index], y_cleaned.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36d2a1-177e-4298-b800-ff3103784124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:53.439554Z",
     "start_time": "2024-02-17T11:33:47.535062Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "\n",
    "# First, we construct a Pipeline comprising two key components: a Scaler and a KNN Classifier Model.\n",
    "# Utilizing a Pipeline streamlines the preprocessing and modeling process. Specifically, after the Scaler is fitted to the training data,\n",
    "# it can be directly applied to the test data without the need for re-fitting. This approach ensures consistency in data preprocessing\n",
    "# between training and testing phases, which is crucial for evaluating the model's performance on new, unseen data.\n",
    "# Adhering to this practice prevents data leakage and ensures that our model's performance metrics accurately reflect its ability to generalize to new data.\n",
    "estimator = Pipeline([('Scaler',MinMaxScaler()),('KNN',KNeighborsClassifier())])\n",
    "\n",
    "# Then we define a parameter grid to search over with grid search techniques\n",
    "param_grid = {'KNN__n_neighbors':[3,5,7,9,11,13,15]}\n",
    "\n",
    "# We create our model selector using Grid search and Cross validations with 10 folds\n",
    "gscv = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Then we fit our models using train dataset and keep track on computational time\n",
    "start_time = datetime.now()\n",
    "gscv.fit(Xtrain,ytrain['target'])\n",
    "total_time = datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca869f0-0497-4700-92c8-acfe992e1363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:53.461074Z",
     "start_time": "2024-02-17T11:33:53.445922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's check what was considered the best set of parameters:\n",
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224c200-a9fe-4ea2-beb1-1cb3189c183a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:33:53.978683Z",
     "start_time": "2024-02-17T11:33:53.463547Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model's performance on the test dataset, focusing on metrics such as accuracy, F1 score, ROC AUC, and the confusion matrix.\n",
    "\n",
    "# Reporting computational time for model training and grid search\n",
    "print(f'Total Time: \\t{total_time} seconds')\n",
    "\n",
    "# Displaying the average accuracy obtained from cross-validation\n",
    "print(f\"Avg acc (CV):\\t{gscv.best_score_:.4f}\")\n",
    "\n",
    "# Fitting the best model found via grid search to the training data\n",
    "best_estimator = gscv.best_estimator_\n",
    "best_estimator.fit(Xtrain, ytrain['target'])\n",
    "\n",
    "# Predicting class labels for the test set\n",
    "ypred = best_estimator.predict(Xtest)\n",
    "\n",
    "# Obtaining probability estimates for the test set (used for ROC AUC calculation)\n",
    "yprob = best_estimator.predict_proba(Xtest)[:, 1]  # Extracting probabilities for the positive class\n",
    "\n",
    "# Computing and displaying accuracy and F1 score for the test set\n",
    "print(f'Acc (Test): \\t{accuracy_score(ytest[\"target\"], ypred):.4f}')\n",
    "print(f'F1 (Test): \\t{f1_score(ytest[\"target\"], ypred):.4f}')\n",
    "\n",
    "# Calculating and printing the ROC AUC score\n",
    "roc_auc = roc_auc_score(ytest[\"target\"], yprob)\n",
    "print(f'AUC: \\t\\t{roc_auc:.4f}')\n",
    "\n",
    "# Generating and displaying the confusion matrix\n",
    "cm = confusion_matrix(ytest[\"target\"], ypred)\n",
    "display = ConfusionMatrixDisplay(cm)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a673d-fd22-4cd1-bb81-2ae83c36d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check graphcally which examples couldn't correctly classified by our model\n",
    "errors = ytest['target'][ytest['target'] != ypred].index\n",
    "print_pca(X_cleaned,y_cleaned,highlight_index=errors,text='Errors: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a65b9-1378-46b9-b4f4-d1bfa4e8b7f9",
   "metadata": {},
   "source": [
    "#### Answer 3:\n",
    "##### As showed by the results, accuracy and F1 scores have values around 0.97 which can be seen as good in Test dataset\n",
    "##### We check F1 score to evaluate the model in inbalanced data, as accuracy can be biased on most common class predicions\n",
    "##### We also checked the ROC AUC and the confision matrix and all metrics seen to be good for a prediction model\n",
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ced1e3-1d64-4be2-a888-182fbad39956",
   "metadata": {},
   "source": [
    "#### 4) Read the Scikit-learn library documentation and configure the automated validation script so that the GridSearchCV() function uses leave-one-out cross-validation instead of k-fold. Which conclusions can be drawn from the mean cross-validation scores and the test scores using a k-neighbor model with optimized k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928ebf8-292f-4f4a-ac7d-4265e1ab16df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:13.416207Z",
     "start_time": "2024-02-17T11:33:53.914938Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "gscv_loo = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=LeaveOneOut(),\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "gscv_loo.fit(Xtrain,ytrain['target'])\n",
    "total_time = datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78020e6-fa4d-41e2-98d2-6b7933846621",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_loo.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a497c-f253-41a2-8d53-d0e6a9319ee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:13.468467Z",
     "start_time": "2024-02-17T11:34:13.428853Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(gscv_loo.cv_results_)\n",
    "results['mean_test_score']\n",
    "#results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5c9f4-3e5c-4992-9c2f-b0f1aedf8e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:13.865816Z",
     "start_time": "2024-02-17T11:34:13.446150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test dataset, focusing on metrics such as accuracy, F1 score, ROC AUC, and the confusion matrix.\n",
    "\n",
    "# Reporting computational time for model training and grid search\n",
    "print(f'Total Time: \\t{total_time} seconds')\n",
    "\n",
    "# Displaying the average accuracy obtained from cross-validation\n",
    "print(f\"Avg acc (CV):\\t{gscv_loo.best_score_:.4f}\")\n",
    "\n",
    "# Fitting the best model found via grid search to the training data\n",
    "best_estimator = gscv_loo.best_estimator_\n",
    "best_estimator.fit(Xtrain, ytrain['target'])\n",
    "\n",
    "# Predicting class labels for the test set\n",
    "ypred = best_estimator.predict(Xtest)\n",
    "\n",
    "# Obtaining probability estimates for the test set (used for ROC AUC calculation)\n",
    "yprob = best_estimator.predict_proba(Xtest)[:, 1]  # Extracting probabilities for the positive class\n",
    "\n",
    "# Computing and displaying accuracy and F1 score for the test set\n",
    "print(f'Acc (Test): \\t{accuracy_score(ytest[\"target\"], ypred):.4f}')\n",
    "print(f'F1 (Test): \\t{f1_score(ytest[\"target\"], ypred):.4f}')\n",
    "\n",
    "# Calculating and printing the ROC AUC score\n",
    "roc_auc = roc_auc_score(ytest[\"target\"], yprob)\n",
    "print(f'AUC: \\t\\t{roc_auc:.4f}')\n",
    "\n",
    "# Generating and displaying the confusion matrix\n",
    "cm = confusion_matrix(ytest[\"target\"], ypred)\n",
    "display = ConfusionMatrixDisplay(cm)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3413f-293c-41e6-ad44-acb3f77d15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check graphcally which examples couldn't correctly classified by our model\n",
    "errors = ytest[\"target\"][ytest[\"target\"] != ypred].index\n",
    "print_pca(X_cleaned,y_cleaned,highlight_index=errors,text='Errors: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387f1ce-2e0a-43ab-8726-67d83098c460",
   "metadata": {},
   "source": [
    "#### Answer 4:\n",
    "##### answer\n",
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c4b98-9e49-473d-865b-44da38523572",
   "metadata": {},
   "source": [
    "#### 5) Elaborate on the need for stratifying the cross-validation process analyzing the distribution of samples by class. If so, please show with empirical evidence what could occur if such a stratification was not performed, specially when decreasing the number of samples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6521e-ee8e-4ce2-9dab-debc69903251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:14.065451Z",
     "start_time": "2024-02-17T11:34:13.869869Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_stratified = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "gscv_strat = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv_stratified,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "gscv_strat.fit(Xtrain,ytrain[\"target\"])\n",
    "total_time = datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde70137-833c-47b0-a277-eb4e284cf8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_strat.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de329e3-2521-485d-a420-20d187ba210a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:14.398027Z",
     "start_time": "2024-02-17T11:34:14.073362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test dataset, focusing on metrics such as accuracy, F1 score, ROC AUC, and the confusion matrix.\n",
    "\n",
    "# Reporting computational time for model training and grid search\n",
    "print(f'Total Time: \\t{total_time} seconds')\n",
    "\n",
    "# Displaying the average accuracy obtained from cross-validation\n",
    "print(f\"Avg acc (CV):\\t{gscv_strat.best_score_:.4f}\")\n",
    "\n",
    "# Fitting the best model found via grid search to the training data\n",
    "best_estimator = gscv_strat.best_estimator_\n",
    "best_estimator.fit(Xtrain, ytrain['target'])\n",
    "\n",
    "# Predicting class labels for the test set\n",
    "ypred = best_estimator.predict(Xtest)\n",
    "\n",
    "# Obtaining probability estimates for the test set (used for ROC AUC calculation)\n",
    "yprob = best_estimator.predict_proba(Xtest)[:, 1]  # Extracting probabilities for the positive class\n",
    "\n",
    "# Computing and displaying accuracy and F1 score for the test set\n",
    "print(f'Acc (Test): \\t{accuracy_score(ytest[\"target\"], ypred):.4f}')\n",
    "print(f'F1 (Test): \\t{f1_score(ytest[\"target\"], ypred):.4f}')\n",
    "\n",
    "# Calculating and printing the ROC AUC score\n",
    "roc_auc = roc_auc_score(ytest[\"target\"], yprob)\n",
    "print(f'AUC: \\t\\t{roc_auc:.4f}')\n",
    "\n",
    "# Generating and displaying the confusion matrix\n",
    "cm = confusion_matrix(ytest[\"target\"], ypred)\n",
    "display = ConfusionMatrixDisplay(cm)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16ead6-1cb8-47ff-86e7-ce87362134ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check graphcally which examples couldn't correctly classified by our model\n",
    "errors = ytest[\"target\"][ytest[\"target\"] != ypred].index\n",
    "print_pca(X_cleaned,y_cleaned,highlight_index=errors,text='Errors: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8e00f-f5fb-43ca-9735-e3738877840d",
   "metadata": {},
   "source": [
    "#### Answer 5:\n",
    "##### In inbalanced datasets like this it's important to take the stratification into account during the cross validation process.\n",
    "##### Cross validation is a process that randomly select a portion of the data to perform training and the rest leaves to evaluate the model.\n",
    "##### In inbalanced data, when selecting the inbalanced dataset it's likely to select a training dataset without examples of all classes.\n",
    "##### In binary classification like this, it's possible to select only one class for validation dataset and end up evaluating our model only in one class.\n",
    "##### This validation dataset don't correspond to the real world data and the best model selected during cross validation will not perform well in production.\n",
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331307b-a300-4705-80e8-4aeca0bd914f",
   "metadata": {},
   "source": [
    "#### 6) Include in the set of hyper-parameters adjusted via cross-validation process the weights of the distance metric between samples according to the “weights” parameter of the model in Scikit-learn. Compute the model’s performance when distance metric weights are fine-tuned within cross-validation with respect to only tuning the number of neighbors (K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7939847-8835-4635-85a8-999bbb0c5078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:14.405044Z",
     "start_time": "2024-02-17T11:34:14.400307Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'KNN__weights':['uniform','distance'],\n",
    "               'KNN__n_neighbors':[3,5,7,9,11]}\n",
    "\n",
    "param_grid_k = {'KNN__n_neighbors':[3,5,7,9,11]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f691c-8ea0-49bf-9dfe-252a9c39dde5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:14.603472Z",
     "start_time": "2024-02-17T11:34:14.409875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only tunning the model on n_neighbors\n",
    "\n",
    "cv_stratified = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "gscv_only_k = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid_k,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv_stratified,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "gscv_only_k.fit(Xtrain,ytrain[\"target\"])\n",
    "total_time = datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92caaf-df95-4b83-aeb1-ce4d6b2f4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_only_k.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ece82-f077-4e40-b891-12e972b8399b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:14.920877Z",
     "start_time": "2024-02-17T11:34:14.610103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test dataset, focusing on metrics such as accuracy, F1 score, ROC AUC, and the confusion matrix.\n",
    "\n",
    "# Reporting computational time for model training and grid search\n",
    "print(f'Total Time: \\t{total_time} seconds')\n",
    "\n",
    "# Displaying the average accuracy obtained from cross-validation\n",
    "print(f\"Avg acc (CV):\\t{gscv_only_k.best_score_:.4f}\")\n",
    "\n",
    "# Fitting the best model found via grid search to the training data\n",
    "best_estimator = gscv_only_k.best_estimator_\n",
    "best_estimator.fit(Xtrain, ytrain['target'])\n",
    "\n",
    "# Predicting class labels for the test set\n",
    "ypred = best_estimator.predict(Xtest)\n",
    "\n",
    "# Obtaining probability estimates for the test set (used for ROC AUC calculation)\n",
    "yprob = best_estimator.predict_proba(Xtest)[:, 1]  # Extracting probabilities for the positive class\n",
    "\n",
    "# Computing and displaying accuracy and F1 score for the test set\n",
    "print(f'Acc (Test): \\t{accuracy_score(ytest[\"target\"], ypred):.4f}')\n",
    "print(f'F1 (Test): \\t{f1_score(ytest[\"target\"], ypred):.4f}')\n",
    "\n",
    "# Calculating and printing the ROC AUC score\n",
    "roc_auc = roc_auc_score(ytest[\"target\"], yprob)\n",
    "print(f'AUC: \\t\\t{roc_auc:.4f}')\n",
    "\n",
    "# Generating and displaying the confusion matrix\n",
    "cm = confusion_matrix(ytest[\"target\"], ypred)\n",
    "display = ConfusionMatrixDisplay(cm)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4cb14c-cb34-4db7-b0ce-99b5e7992ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:15.246882Z",
     "start_time": "2024-02-17T11:34:14.925093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tunning the model in all parameters\n",
    "\n",
    "cv_stratified = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "gscv_with_weights = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv_stratified,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "gscv_with_weights.fit(Xtrain,ytrain[\"target\"])\n",
    "total_time = datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb7ca2-508d-44a8-b7bf-86048d547821",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_with_weights.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4963169-3076-4cdc-a7bb-3ee2896d9dae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:15.569321Z",
     "start_time": "2024-02-17T11:34:15.257074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test dataset, focusing on metrics such as accuracy, F1 score, ROC AUC, and the confusion matrix.\n",
    "\n",
    "# Reporting computational time for model training and grid search\n",
    "print(f'Total Time: \\t{total_time} seconds')\n",
    "\n",
    "# Displaying the average accuracy obtained from cross-validation\n",
    "print(f\"Avg acc (CV):\\t{gscv_with_weights.best_score_:.4f}\")\n",
    "\n",
    "# Fitting the best model found via grid search to the training data\n",
    "best_estimator = gscv_with_weights.best_estimator_\n",
    "best_estimator.fit(Xtrain, ytrain['target'])\n",
    "\n",
    "# Predicting class labels for the test set\n",
    "ypred = best_estimator.predict(Xtest)\n",
    "\n",
    "# Obtaining probability estimates for the test set (used for ROC AUC calculation)\n",
    "yprob = best_estimator.predict_proba(Xtest)[:, 1]  # Extracting probabilities for the positive class\n",
    "\n",
    "# Computing and displaying accuracy and F1 score for the test set\n",
    "print(f'Acc (Test): \\t{accuracy_score(ytest[\"target\"], ypred):.4f}')\n",
    "print(f'F1 (Test): \\t{f1_score(ytest[\"target\"], ypred):.4f}')\n",
    "\n",
    "# Calculating and printing the ROC AUC score\n",
    "roc_auc = roc_auc_score(ytest[\"target\"], yprob)\n",
    "print(f'AUC: \\t\\t{roc_auc:.4f}')\n",
    "\n",
    "# Generating and displaying the confusion matrix\n",
    "cm = confusion_matrix(ytest[\"target\"], ypred)\n",
    "display = ConfusionMatrixDisplay(cm)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2667590-56ab-4d00-8b09-052b7f172327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check graphcally which examples couldn't correctly classified by our model\n",
    "errors = ytest[\"target\"][ytest[\"target\"] != ypred].index\n",
    "print_pca(X_cleaned,y_cleaned,list(errors),text='Errors: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfac42-ca7b-481c-96bc-4d629d38ab2f",
   "metadata": {},
   "source": [
    "#### Answer 6:\n",
    "##### answer\n",
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba88633-1ae8-4bcf-bb2d-2a80879ff81c",
   "metadata": {},
   "source": [
    "#### 7) Following the same approach as in the last section, enter the type of distance metric (“metric” parameter) within the cross-validation process. Evaluates the results and gains / losses of generalizability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1d5a9-7b02-435f-aac3-1972af12edec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:15.594071Z",
     "start_time": "2024-02-17T11:34:15.568425Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'KNN__weights':['uniform','distance'],\n",
    "               'KNN__metric':['cityblock','minkowski','cosine'],\n",
    "               'KNN__p':[2,3,4,5,6,7],\n",
    "               'KNN__n_neighbors':[3,5,7,9,11,13,15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2c816-5b04-404c-b792-d2d7a0ddb489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:22.467301Z",
     "start_time": "2024-02-17T11:34:15.578283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tunning the model in all parameters\n",
    "\n",
    "cv_stratified = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "gscv_complete = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv_stratified,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "gscv_complete.fit(Xtrain,ytrain['target'])\n",
    "total_time = datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51f657-3de2-45ec-8d04-e53eb0e44692",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_complete.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ded03-df80-4d77-9b71-92d5559ee4e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T11:34:22.888695Z",
     "start_time": "2024-02-17T11:34:22.476630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test dataset, focusing on metrics such as accuracy, F1 score, ROC AUC, and the confusion matrix.\n",
    "\n",
    "# Reporting computational time for model training and grid search\n",
    "print(f'Total Time: \\t{total_time} seconds')\n",
    "\n",
    "# Displaying the average accuracy obtained from cross-validation\n",
    "print(f\"Avg acc (CV):\\t{gscv_complete.best_score_:.4f}\")\n",
    "\n",
    "# Fitting the best model found via grid search to the training data\n",
    "best_estimator = gscv_complete.best_estimator_\n",
    "best_estimator.fit(Xtrain, ytrain['target'])\n",
    "\n",
    "# Predicting class labels for the test set\n",
    "ypred = best_estimator.predict(Xtest)\n",
    "\n",
    "# Obtaining probability estimates for the test set (used for ROC AUC calculation)\n",
    "yprob = best_estimator.predict_proba(Xtest)[:, 1]  # Extracting probabilities for the positive class\n",
    "\n",
    "# Computing and displaying accuracy and F1 score for the test set\n",
    "print(f'Acc (Test): \\t{accuracy_score(ytest[\"target\"], ypred):.4f}')\n",
    "print(f'F1 (Test): \\t{f1_score(ytest[\"target\"], ypred):.4f}')\n",
    "\n",
    "# Calculating and printing the ROC AUC score\n",
    "roc_auc = roc_auc_score(ytest[\"target\"], yprob)\n",
    "print(f'AUC: \\t\\t{roc_auc:.4f}')\n",
    "\n",
    "# Generating and displaying the confusion matrix\n",
    "cm = confusion_matrix(ytest[\"target\"], ypred)\n",
    "display = ConfusionMatrixDisplay(cm)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8598ae-53ef-4044-b78d-b1c09464b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check graphcally which examples couldn't correctly classified by our model\n",
    "errors = ytest[\"target\"][ytest[\"target\"] != ypred].index\n",
    "print_pca(X_cleaned,y_cleaned,list(errors),text='Errors: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab107596-3c9d-49b8-8a6b-18b2452d80b2",
   "metadata": {},
   "source": [
    "#### Answer 7:\n",
    "##### answer\n",
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b7a2a-258a-42c0-908f-496346e0cb0b",
   "metadata": {},
   "source": [
    "#### Now let's print Train dataset and add the prediction errors on the plot to analyse if the train dataset has some different pattern that is causing this errors in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550a13d-06fb-4c24-b27e-85c880419cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print only test dataset with correspond errors\n",
    "errors = ytest[\"target\"][ytest[\"target\"] != ypred].index\n",
    "errors_points = Xtest.loc[errors]\n",
    "print_pca(Xtrain,ytrain,highlight_points=errors_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb524b-1502-45ad-a1ca-62743faa2ae0",
   "metadata": {},
   "source": [
    "#### Now let's check another dimension reducer to see if we have different conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53deb8-6fcc-473e-941f-319a5d971037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see using StandardScaler()\n",
    "print_pca(X_cleaned,y_cleaned,highlight_index=errors,scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c744430-f01a-493d-9daf-4eee4d076265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Now let's check with TSNE transformer\n",
    "print_pca(X_cleaned,y_cleaned,highlight_index=errors,dim_reducer=TSNE(n_components=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57334b93-93c2-49d6-a5d5-f78b3d312574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see with StandardScaler and TSNE\n",
    "print_pca(X_cleaned,y_cleaned,highlight_index=errors,dim_reducer=TSNE(n_components=2),scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f0f79-371f-446b-a462-010c8227cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "print_pca(X_cleaned,y_cleaned,highlight_index=errors,dim_reducer=TSNE(n_components=2),scaler=MaxAbsScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9edaff-aea2-4016-8181-354e3722f013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
