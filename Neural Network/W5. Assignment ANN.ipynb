{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab assignment: perceptron training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted by: Jorge de la Torre Garcia (26260320R), Lydia Phoebe Amanda Lilius (DNI), Miguel Galán Cisneros (77665422B), Vitor Oliveira de Souza (Z0963220P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will learn how perceptrons work and are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
    "\n",
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td>This is an advanced and voluntary exercise that can help you gain a deeper knowledge into the topic. Good luck!</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "During the assignment you will make use of several Python packages that might not be installed in your machine. If that is the case, you can install new Python packages with\n",
    "\n",
    "    conda install PACKAGENAME\n",
    "    \n",
    "if you are using Python Anaconda. Else you should use\n",
    "\n",
    "    pip install PACKAGENAME\n",
    "\n",
    "You will need the following packages for this particular assignment. Make sure they are available before proceeding:\n",
    "\n",
    "* **numpy**\n",
    "* **scikit-learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AND and OR problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the AND and OR problems in the **dataset** form we will be using throughout this assignment. A dataset is composed of two matrices X and Y, storing respectively the **inputs** fed to the networks and the desired **outputs** or **targets** for such inputs. We will use numpy's arrays for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_and = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "Y_and = np.array([[0], [0], [0], [1]])\n",
    "X_or = X_and.copy()    # same inputs as for AND\n",
    "Y_or = np.array([[0], [1], [1], [1]])\n",
    "print(X_and)\n",
    "print(Y_and)\n",
    "print(X_or)\n",
    "print(Y_or)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the patterns above we have prepended a 1, so that the **weights** **w** also include the **bias** term b and a dot product of the form **w**·**x** actually computes **w**·**x** + b. Hence, in this particular case **w** = (b, w1, w2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in the theory, **perceptrons** are based on the **McCulloch-Pitts neuron**, which is a simplified version of a neuron in the human brain. The **activation function** of this neuron is 1 when its inputs are greater than or equal to 0, and 0 otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def step_activation(x):\n",
    "    return 1*(x >= 0)   # multiply by 1 to change from boolean to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Figure out by yourself some values for <b>w</b> which solve the AND and OR problems. Store them in 2 variables called <b>w_and</b> and <b>w_or</b>.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "It may help if you print the points in (x1, x2) axes and interpret <b>w</b> and b as a hyperplane.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'AND problem')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x1=[0,0,1,1]\n",
    "x2=[0,1,0,1]\n",
    "plt.scatter(x1,x2, color=['r','r','r','g'])\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('AND problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_and = np.array([-1.5,1,1])\n",
    "step_activation(X_and.dot(w_and))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'OR problem')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsnElEQVR4nO3dfVTVVb7H8c+BIw8+wIgkYCBqjxSmCWmiTmlJg42mc52wLB8SJ3NMTasbw11WLmfoeZkmaqU2zlWHsrInMmn5hEoWXmyVNtVNDVTQoATUQoF9//B6piOoYHAO7N6vtX5rzdln/377e/Zi/H3av9/5HYcxxggAAMASPt4uAAAAoDERbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuANTpo48+0h//+EdFRETIz89P4eHhGjlypHJzc2v1feWVV+RwOFyb0+lURESERo0apa+//toL1Z/b6Xrz8vLO23fcuHHq0qVL0xcFoNEQbgDUMn/+fPXr10/79+/XU089pQ8//FDPPPOMDhw4oP79++uFF16oc79ly5YpNzdXH374oaZMmaK3335b/fv31w8//ODhTwDg18zp7QIANC9bt27V9OnTNWTIEL355ptyOv/9z8SoUaM0YsQITZs2Tddee6369evntm9sbKzi4+MlSTfeeKOqq6v16KOPas2aNRo/fnyT1WyM0U8//aTAwMAmGwNAy8HKDQA36enpcjgcWrhwoVuwkSSn06mMjAw5HA498cQT5z3W6aBz6NCh8/YdN26c2rZtq127dummm25SmzZtdNFFF2nKlCk6fvy4W1+Hw6EpU6Zo0aJFiomJkb+/v/7+979LkrZs2aKbbrpJ7dq1U+vWrZWQkKD33nuvzjF/+OEHjR8/XiEhIWrTpo2GDh2qPXv2nLdWY4wyMjLUs2dPBQYGqn379ho5cmStfW+88UbFxsYqNzdXCQkJCgwMVJcuXbRs2TJJ0nvvvadevXqpdevW6t69u9auXXvesQGcH+EGgEt1dbU2bNig+Ph4RUZG1tknKipKcXFxWr9+vaqrq895vL1790qSLr/88nqNf/LkSQ0ZMkQ33XST1qxZoylTpmjx4sVKTk6u1XfNmjVauHChZs2apQ8++EADBgzQpk2bNGjQIJWVlWnJkiVatWqV2rVrp6FDhyozM7PWMSZMmCAfHx+tXLlSc+fO1ccff6wbb7xRR44cOWed9957r6ZPn66bb75Za9asUUZGhnbt2qWEhIRaQa64uFjjx49XSkqK3nrrLXXv3l333HOPZs+erdTUVD388MN6/fXX1bZtWw0fPlwHDx6s11wBOAcDAP+vuLjYSDKjRo06Z7/k5GQjyRw6dMgYY8yyZcuMJPPRRx+ZkydPmoqKCrN27VoTHh5ufvvb35qTJ0+ed+yxY8caSeb55593a//rX/9qJJktW7a42iSZ4OBg8/3337v1vf76603Hjh1NRUWFq62qqsrExsaayMhIU1NT41bviBEj3PbfunWrkWTmzJnjVld0dLTrdW5urpFknn32Wbd9CwsLTWBgoHn44YddbTfccIORZPLy8lxtpaWlxtfX1wQGBpoDBw642nfu3GkkmXnz5p13rgCcGys3ABrMGCPp1OWhn7v++uvVqlUrtWvXTr/73e/Uvn17vfXWW7Uub53L6NGj3V7feeedkqQNGza4tQ8aNEjt27d3vT527Ji2b9+ukSNHqm3btq52X19f3X333dq/f7++/PLLc46VkJCg6OjoWmP93LvvviuHw6G77rpLVVVVri08PFw9evTQxo0b3fpHREQoLi7O9TokJEQdO3ZUz5491alTJ1d7TEyMJOnbb78969gA6ocbigG4hIaGqnXr1q7LSWezb98+tW7dWiEhIW7ty5cvV0xMjCoqKpSZmanFixfrjjvu0Pvvv1+v8Z1Opzp06ODWFh4eLkkqLS11a4+IiHB7/cMPP8gYU6tdkitEnHmM08c+s+3Mfj936NAhGWMUFhZW5/vdunVze33mHEmSn59frXY/Pz9J0k8//XTWsQHUD+EGgIuvr68GDhyotWvXav/+/XXed7N//37t2LFDSUlJ8vX1dXsvJibGdRPxwIEDVV1drZdfflmrV6/WyJEjzzt+VVWVSktL3QJOcXGxJNUKPWeuGrVv314+Pj4qKiqqddzT97GEhoa6tZ8+9pltl1566VlrDA0NlcPhUE5Ojvz9/Wu9X1cbAM/ishQAN6mpqTLGaPLkybVuGK6urtZ9990nY4xSU1PPe6ynnnpK7du316xZs1RTU1Ov8VesWOH2euXKlZJOffPoXNq0aaM+ffrojTfe0I8//uhqr6mp0X//938rMjKy1o3NZ461bds2ffvtt+cc6/e//72MMTpw4IDi4+Nrbd27d6/HpwTQlFi5AeCmX79+mjt3rqZPn67+/ftrypQp6ty5swoKCrRgwQJt375dc+fOVUJCwnmP1b59e9c3glauXKm77rrrnP39/Pz07LPP6ujRo7ruuuu0bds2zZkzR0lJSerfv/95x0tPT9fgwYM1cOBAPfjgg/Lz81NGRoY+//xzrVq1qtZqT15enlJSUvTHP/5RhYWFSktL08UXX6zJkyefc37+9Kc/afz48crLy9Nvf/tbtWnTRkVFRdqyZYu6d++u++6777y1Amg6rNwAqOX+++/X1q1bFRkZqZkzZ2rQoEGaMWOGIiIitGXLFt1///0NOlbnzp01e/bs8351vFWrVnr33XeVnZ2t2267TfPmzdPEiRP12muv1WusG264QevXr1ebNm00btw4jRo1SmVlZXr77bfr/Dr5kiVLdOLECY0aNUpTp05VfHy8Nm7cWOd9Mj+3ePFivfDCC9q8ebNGjRqlW2+9VbNmzdKxY8fUu3fvetUKoOk4zOmvPQCAF40bN06rV6/W0aNHvV0KgBaOlRsAAGAVwg0AALAKl6UAAIBVWLkBAABWIdwAAACrEG4AAIBVfnUP8aupqdHBgwfVrl27Wg/0AgAAzZMxRhUVFerUqZN8fM69NvOrCzcHDx5UVFSUt8sAAAAXoLCwsM7fvfu5X124adeunaRTkxMUFOTlagAAQH2Ul5crKirKdR4/l19duDl9KSooKIhwAwBAC1OfW0q4oRgAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVw0wg27N2g2/55m0KeDFHoU6Ea/cZofXLgE2+XBQCARxhj9M6X72jwPwar/ZPt1fHpjkp5O0WfH/7cK/V4Ndxs3rxZQ4cOVadOneRwOLRmzZrz7rNp0ybFxcUpICBA3bp106JFi5q+0HP4W87fNGj5IGV9laUffvpBpT+W6tVdr6rPy320NH+pV2sDAKCpGWM0be00DfvnMG3Yu0FHfjqi745/p79/+nddu/havfWvtzxek1fDzbFjx9SjRw+98MIL9eq/d+9eDRkyRAMGDFB+fr7+8pe/aOrUqXr99debuNK6bf52s9LWp0mSqkyVq72qpkpGRhPfmagvS770Sm0AAHjC6t2rNf/j+ZKkalPtaq+qqVJ1TbWSVyfr8LHDHq3Jq+EmKSlJc+bM0R/+8Id69V+0aJE6d+6suXPnKiYmRikpKbrnnnv0zDPPNHGldZu3fZ6cPmf/7VEf+Whh3kIPVgQAgGfN/WiufB2+db5nZHSy5qTHr2S0qHtucnNzlZiY6NZ2yy23KC8vTydPnqxzn8rKSpWXl7ttjWVLwRZV1VSd9f0qU6Wcb3MabTwAAJoTY4y2H9jutmJzphpTo9zCXA9W1cLCTXFxscLCwtzawsLCVFVVpZKSkjr3SU9PV3BwsGuLiopqtHp8fepOqj/n9D37yg4AAC2dj+PcUcIhR73Ol42pRYUbSXI4HG6vjTF1tp+WmpqqsrIy11ZYWNhotSRdmnTuy1IOHyVdmtRo4wEA0Jw4HA4lXpJ41stSpw3uNthDFZ3SosJNeHi4iouL3doOHz4sp9OpDh061LmPv7+/goKC3LbGMq3PNFe4OpNDDvn7+utPcX9qtPEAAGhuZvadedbLUr4OX4UEhuiua+7yaE0tKtz07dtX2dnZbm3r1q1TfHy8WrVq5fF6uod114o/rJDTx+mWWn0cPgp0BuqdO95Rp3adPF4XAACeMrDrQC0YskAOOdyuZjjkUJB/kNbetVbt/Nt5tCav3hBy9OhR/e///q/r9d69e7Vz506FhISoc+fOSk1N1YEDB7R8+XJJ0qRJk/TCCy9oxowZmjhxonJzc7VkyRKtWrXKWx9BybHJ6hPZR4vzFiunIEc+Dh8lXpKolF4pCm8b7rW6AADwlMnXTdagroO0KG+Rth/YLn9ffw27YpjG9RynkMAQj9fjMGe7ruIBGzdu1MCBA2u1jx07Vq+88orGjRunffv2aePGja73Nm3apAceeEC7du1Sp06d9J//+Z+aNGlSvccsLy9XcHCwysrKGvUSFQAAaDoNOX97Ndx4A+EGAICWpyHn7xZ1zw0AAMD5EG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFbxerjJyMhQ165dFRAQoLi4OOXk5Jyz/4oVK9SjRw+1bt1aERERGj9+vEpLSz1ULQAAaO68Gm4yMzM1ffp0paWlKT8/XwMGDFBSUpIKCgrq7L9lyxaNGTNGEyZM0K5du/Taa6/pk08+UUpKiocrBwAAzZVXw81zzz2nCRMmKCUlRTExMZo7d66ioqK0cOHCOvt/9NFH6tKli6ZOnaquXbuqf//+uvfee5WXl+fhygEAQHPltXBz4sQJ7dixQ4mJiW7tiYmJ2rZtW537JCQkaP/+/crKypIxRocOHdLq1at16623nnWcyspKlZeXu20AAMBeXgs3JSUlqq6uVlhYmFt7WFiYiouL69wnISFBK1asUHJysvz8/BQeHq7f/OY3mj9//lnHSU9PV3BwsGuLiopq1M8BAACaF6/fUOxwONxeG2NqtZ22e/duTZ06VbNmzdKOHTu0du1a7d27V5MmTTrr8VNTU1VWVubaCgsLG7V+AADQvDi9NXBoaKh8fX1rrdIcPny41mrOaenp6erXr58eeughSdI111yjNm3aaMCAAZozZ44iIiJq7ePv7y9/f//G/wAAAKBZ8trKjZ+fn+Li4pSdne3Wnp2drYSEhDr3OX78uHx83Ev29fWVdGrFBwAAwKuXpWbMmKGXX35ZS5cu1RdffKEHHnhABQUFrstMqampGjNmjKv/0KFD9cYbb2jhwoXas2ePtm7dqqlTp6p3797q1KmTtz4GAABoRrx2WUqSkpOTVVpaqtmzZ6uoqEixsbHKyspSdHS0JKmoqMjtmTfjxo1TRUWFXnjhBc2cOVO/+c1vNGjQID355JPe+ggAAKCZcZhf2fWc8vJyBQcHq6ysTEFBQd4uBwAA1ENDzt9e/7YUAABAYyLcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACs4vVwk5GRoa5duyogIEBxcXHKyck5Z//KykqlpaUpOjpa/v7+uuSSS7R06VIPVQsAAJo7pzcHz8zM1PTp05WRkaF+/fpp8eLFSkpK0u7du9W5c+c697n99tt16NAhLVmyRJdeeqkOHz6sqqoqD1cOAACaK4cxxnhr8D59+qhXr15auHChqy0mJkbDhw9Xenp6rf5r167VqFGjtGfPHoWEhFzQmOXl5QoODlZZWZmCgoIuuHYAAOA5DTl/e+2y1IkTJ7Rjxw4lJia6tScmJmrbtm117vP2228rPj5eTz31lC6++GJdfvnlevDBB/Xjjz+edZzKykqVl5e7bQAAwF5euyxVUlKi6upqhYWFubWHhYWpuLi4zn327NmjLVu2KCAgQG+++aZKSko0efJkff/992e97yY9PV2PP/54o9cPAACaJ6/fUOxwONxeG2NqtZ1WU1Mjh8OhFStWqHfv3hoyZIiee+45vfLKK2ddvUlNTVVZWZlrKywsbPTPAAAAmg+vrdyEhobK19e31irN4cOHa63mnBYREaGLL75YwcHBrraYmBgZY7R//35ddtlltfbx9/eXv79/4xYPAACaLa+t3Pj5+SkuLk7Z2dlu7dnZ2UpISKhzn379+ungwYM6evSoq+2rr76Sj4+PIiMjm7ReAADQMnj1stSMGTP08ssva+nSpfriiy/0wAMPqKCgQJMmTZJ06pLSmDFjXP3vvPNOdejQQePHj9fu3bu1efNmPfTQQ7rnnnsUGBjorY8BAACaEa8+5yY5OVmlpaWaPXu2ioqKFBsbq6ysLEVHR0uSioqKVFBQ4Orftm1bZWdn6/7771d8fLw6dOig22+/XXPmzPHWRwAAAM2MV59z4w085wYAgJanRTznBgAAoCkQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFilQeHm008/1Zw5c5SRkaGSkhK398rLy3XPPfc0anEAAAAN5TDGmPp0XLdunYYOHarLLrtMFRUVOn78uF599VUNHDhQknTo0CF16tRJ1dXVTVrwL1VeXq7g4GCVlZUpKCjI2+UAAIB6aMj5u94rN4899pgefPBBff7559q3b58efvhhDRs2TGvXrv3FBQMAADQWZ3077tq1S//4xz8kSQ6HQw899JAiIyM1cuRIrVq1Sr17926yIgEAAOqr3uHG399fR44ccWu744475OPjo1GjRunZZ59t7NoAAAAarN7hpmfPntqwYYPi4uLc2pOTk1VTU6OxY8c2enEAAAANVe9wc99992nz5s11vnfHHXdIkl588cXGqQoAAOAC1TvcjBgxQiNGjNCHH36om2++udb7d9xxh8rLyxu1OAAAgIZq8EP8br31Vs2cOVMnTpxwtX333XcaOnSoUlNTG7U4AACAhmpwuNm8ebPeeecdXXfdddq1a5fee+89xcbG6ujRo/r000+bokYAAIB6a3C46dOnj/Lz83XNNdcoLi5OI0aM0MyZM7V+/XpFRUU1RY0AAAD1dkG/LfXll1/qk08+UWRkpJxOp/71r3/p+PHjjV0bAABAgzU43DzxxBPq27evBg8erM8//1yffPKJayUnNze3KWoEAACotwaHm+eff15r1qzR/PnzFRAQoKuvvloff/yx/vCHP+jGG29sghIBAADqr95fBT/ts88+U2hoqFtbq1at9PTTT+v3v/99oxUGAABwIRq8cnNmsPm5G2644RcVAwAA8Etd0A3FAAAAzRXhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFjF6+EmIyNDXbt2VUBAgOLi4pSTk1Ov/bZu3Sqn06mePXs2bYEAAKBF8Wq4yczM1PTp05WWlqb8/HwNGDBASUlJKigoOOd+ZWVlGjNmjG666SYPVQoAAFoKhzHGeGvwPn36qFevXlq4cKGrLSYmRsOHD1d6evpZ9xs1apQuu+wy+fr6as2aNdq5c2e9xywvL1dwcLDKysoUFBT0S8oHAAAe0pDzt9dWbk6cOKEdO3YoMTHRrT0xMVHbtm07637Lli3TN998o0cffbSpSwQAAC2Q01sDl5SUqLq6WmFhYW7tYWFhKi4urnOfr7/+Wo888ohycnLkdNav9MrKSlVWVrpel5eXX3jRAACg2fP6DcUOh8PttTGmVpskVVdX684779Tjjz+uyy+/vN7HT09PV3BwsGuLior6xTUDAIDmy2vhJjQ0VL6+vrVWaQ4fPlxrNUeSKioqlJeXpylTpsjpdMrpdGr27Nn69NNP5XQ6tX79+jrHSU1NVVlZmWsrLCxsks8DAACaB69dlvLz81NcXJyys7M1YsQIV3t2drZuu+22Wv2DgoL02WefubVlZGRo/fr1Wr16tbp27VrnOP7+/vL392/c4gEAQLPltXAjSTNmzNDdd9+t+Ph49e3bVy+++KIKCgo0adIkSadWXQ4cOKDly5fLx8dHsbGxbvt37NhRAQEBtdoBAMCvl1fDTXJyskpLSzV79mwVFRUpNjZWWVlZio6OliQVFRWd95k3AAAAP+fV59x4A8+5AQCg5WkRz7kBAABoCoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVvB5uMjIy1LVrVwUEBCguLk45OTln7fvGG29o8ODBuuiiixQUFKS+ffvqgw8+8GC1AACgufNquMnMzNT06dOVlpam/Px8DRgwQElJSSooKKiz/+bNmzV48GBlZWVpx44dGjhwoIYOHar8/HwPVw4AAJorhzHGeGvwPn36qFevXlq4cKGrLSYmRsOHD1d6enq9jnH11VcrOTlZs2bNqlf/8vJyBQcHq6ysTEFBQRdUNwAA8KyGnL+9tnJz4sQJ7dixQ4mJiW7tiYmJ2rZtW72OUVNTo4qKCoWEhDRFiQAAoAVyemvgkpISVVdXKywszK09LCxMxcXF9TrGs88+q2PHjun2228/a5/KykpVVla6XpeXl19YwQAAoEXw+g3FDofD7bUxplZbXVatWqXHHntMmZmZ6tix41n7paenKzg42LVFRUX94poBAEDz5bVwExoaKl9f31qrNIcPH661mnOmzMxMTZgwQa+++qpuvvnmc/ZNTU1VWVmZayssLPzFtQMAgObLa+HGz89PcXFxys7OdmvPzs5WQkLCWfdbtWqVxo0bp5UrV+rWW2897zj+/v4KCgpy2wAAgL28ds+NJM2YMUN333234uPj1bdvX7344osqKCjQpEmTJJ1adTlw4ICWL18u6VSwGTNmjJ5//nldf/31rlWfwMBABQcHe+1zAACA5sOr4SY5OVmlpaWaPXu2ioqKFBsbq6ysLEVHR0uSioqK3J55s3jxYlVVVenPf/6z/vznP7vax44dq1deecXT5QMAgGbIq8+58QaecwMAQMvTIp5zAwAA0BQINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuGsOGDdJtt0khIVJoqDR6tPTJJ96uCgAAjzDG6J0v39HgfwxW+yfbq+PTHZXydoo+P/y5V+rxerjJyMhQ165dFRAQoLi4OOXk5Jyz/6ZNmxQXF6eAgAB169ZNixYt8lClZ/G3v0mDBklZWdIPP0ilpdKrr0p9+khLl3q3NgAAmpgxRtPWTtOwfw7Thr0bdOSnI/ru+Hf6+6d/17WLr9Vb/3rL4zV5NdxkZmZq+vTpSktLU35+vgYMGKCkpCQVFBTU2X/v3r0aMmSIBgwYoPz8fP3lL3/R1KlT9frrr3u48v+3ebOUlnbqf1dV/bu9qkoyRpo4UfryS+/UBgCAB6zevVrzP54vSao21a72qpoqVddUK3l1sg4fO+zRmrwabp577jlNmDBBKSkpiomJ0dy5cxUVFaWFCxfW2X/RokXq3Lmz5s6dq5iYGKWkpOiee+7RM8884+HK/9+8eZLTefb3fXyks3wWAABsMPejufJ1+Nb5npHRyZqTWprv2SsZXgs3J06c0I4dO5SYmOjWnpiYqG3bttW5T25ubq3+t9xyi/Ly8nTy5Mk696msrFR5ebnb1mi2bHFfsTlTVZV0nstsAAC0VMYYbT+w3W3F5kw1pka5hbkerMqL4aakpETV1dUKCwtzaw8LC1NxcXGd+xQXF9fZv6qqSiUlJXXuk56eruDgYNcWFRXVOB9AknzrTqpuzrWyAwBAC+fjOHeUcMghX596nC8bkddvKHY4HG6vjTG12s7Xv67201JTU1VWVubaCgsLf2HFP5OUdP7LUklJjTceAADNiMPhUOIliWe9LHXa4G6DPVTRKV4LN6GhofL19a21SnP48OFaqzOnhYeH19nf6XSqQ4cOde7j7++voKAgt63RTJt26sbhujgckr+/9Kc/Nd54AAA0MzP7zjzrZSlfh69CAkN01zV3ebQmr4UbPz8/xcXFKTs72609OztbCQkJde7Tt2/fWv3XrVun+Ph4tWrVqslqPavu3aUVK06t3vz8EpWPjxQYKL3zjtSpk+frAgDAQwZ2HagFQxbIIYecPv++muGQQ0H+QVp711q182/n0Zocxpxt6aHpZWZm6u6779aiRYvUt29fvfjii3rppZe0a9cuRUdHKzU1VQcOHNDy5cslnfoqeGxsrO69915NnDhRubm5mjRpklatWqX/+I//qNeY5eXlCg4OVllZWeOt4uzbJy1efOrmYR8fKTFRSkmRwsMb5/gAADRz/yr5lxblLdL2A9vl7+uvYVcM07ie4xQSGNIox2/I+durd7smJyertLRUs2fPVlFRkWJjY5WVlaXo6GhJUlFRkdszb7p27aqsrCw98MADWrBggTp16qR58+bVO9g0mS5dpPR079YAAIAXXRl6peb+bq63y5Dk5ZUbb2iSlRsAANCkGnL+9vq3pQAAABoT4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsIpXf37BG04/kLm8vNzLlQAAgPo6fd6uzw8r/OrCTUVFhSQpKirKy5UAAICGqqioUHBw8Dn7/Op+W6qmpkYHDx5Uu3bt5HA4GvXY5eXlioqKUmFhIb9b1YSYZ89gnj2DefYc5tozmmqejTGqqKhQp06d5ONz7rtqfnUrNz4+PoqMjGzSMYKCgvg/jgcwz57BPHsG8+w5zLVnNMU8n2/F5jRuKAYAAFYh3AAAAKsQbhqRv7+/Hn30Ufn7+3u7FKsxz57BPHsG8+w5zLVnNId5/tXdUAwAAOzGyg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3DRQRkaGunbtqoCAAMXFxSknJ+ec/Tdt2qS4uDgFBASoW7duWrRokYcqbdkaMs9vvPGGBg8erIsuukhBQUHq27evPvjgAw9W23I19O/5tK1bt8rpdKpnz55NW6AlGjrPlZWVSktLU3R0tPz9/XXJJZdo6dKlHqq25WroPK9YsUI9evRQ69atFRERofHjx6u0tNRD1bZMmzdv1tChQ9WpUyc5HA6tWbPmvPt45TxoUG///Oc/TatWrcxLL71kdu/ebaZNm2batGljvv322zr779mzx7Ru3dpMmzbN7N6927z00kumVatWZvXq1R6uvGVp6DxPmzbNPPnkk+bjjz82X331lUlNTTWtWrUy//M//+PhyluWhs7zaUeOHDHdunUziYmJpkePHp4ptgW7kHkeNmyY6dOnj8nOzjZ79+4127dvN1u3bvVg1S1PQ+c5JyfH+Pj4mOeff97s2bPH5OTkmKuvvtoMHz7cw5W3LFlZWSYtLc28/vrrRpJ58803z9nfW+dBwk0D9O7d20yaNMmt7corrzSPPPJInf0ffvhhc+WVV7q13Xvvveb6669vshpt0NB5rstVV11lHn/88cYuzSoXOs/Jycnmv/7rv8yjjz5KuKmHhs7z+++/b4KDg01paaknyrNGQ+f56aefNt26dXNrmzdvnomMjGyyGm1Tn3DjrfMgl6Xq6cSJE9qxY4cSExPd2hMTE7Vt27Y698nNza3V/5ZbblFeXp5OnjzZZLW2ZBcyz2eqqalRRUWFQkJCmqJEK1zoPC9btkzffPONHn300aYu0QoXMs9vv/224uPj9dRTT+niiy/W5ZdfrgcffFA//vijJ0pukS5knhMSErR//35lZWXJGKNDhw5p9erVuvXWWz1R8q+Gt86Dv7ofzrxQJSUlqq6uVlhYmFt7WFiYiouL69ynuLi4zv5VVVUqKSlRREREk9XbUl3IPJ/p2Wef1bFjx3T77bc3RYlWuJB5/vrrr/XII48oJydHTif/dNTHhczznj17tGXLFgUEBOjNN99USUmJJk+erO+//577bs7iQuY5ISFBK1asUHJysn766SdVVVVp2LBhmj9/vidK/tXw1nmQlZsGcjgcbq+NMbXazte/rna4a+g8n7Zq1So99thjyszMVMeOHZuqPGvUd56rq6t155136vHHH9fll1/uqfKs0ZC/55qaGjkcDq1YsUK9e/fWkCFD9Nxzz+mVV15h9eY8GjLPu3fv1tSpUzVr1izt2LFDa9eu1d69ezVp0iRPlPqr4o3zIP/5VU+hoaHy9fWt9V8Bhw8frpVKTwsPD6+zv9PpVIcOHZqs1pbsQub5tMzMTE2YMEGvvfaabr755qYss8Vr6DxXVFQoLy9P+fn5mjJliqRTJ2FjjJxOp9atW6dBgwZ5pPaW5EL+niMiInTxxRcrODjY1RYTEyNjjPbv36/LLrusSWtuiS5kntPT09WvXz899NBDkqRrrrlGbdq00YABAzRnzhxW1huJt86DrNzUk5+fn+Li4pSdne3Wnp2drYSEhDr36du3b63+69atU3x8vFq1atVktbZkFzLP0qkVm3HjxmnlypVcM6+Hhs5zUFCQPvvsM+3cudO1TZo0SVdccYV27typPn36eKr0FuVC/p779eungwcP6ujRo662r776Sj4+PoqMjGzSeluqC5nn48ePy8fH/RTo6+sr6d8rC/jlvHYebNLblS1z+quGS5YsMbt37zbTp083bdq0Mfv27TPGGPPII4+Yu+++29X/9FfgHnjgAbN7926zZMkSvgpeDw2d55UrVxqn02kWLFhgioqKXNuRI0e89RFahIbO85n4tlT9NHSeKyoqTGRkpBk5cqTZtWuX2bRpk7nssstMSkqKtz5Ci9DQeV62bJlxOp0mIyPDfPPNN2bLli0mPj7e9O7d21sfoUWoqKgw+fn5Jj8/30gyzz33nMnPz3d95b65nAcJNw20YMECEx0dbfz8/EyvXr3Mpk2bXO+NHTvW3HDDDW79N27caK699lrj5+dnunTpYhYuXOjhilumhszzDTfcYCTV2saOHev5wluYhv49/xzhpv4aOs9ffPGFufnmm01gYKCJjIw0M2bMMMePH/dw1S1PQ+d53rx55qqrrjKBgYEmIiLCjB492uzfv9/DVbcsGzZsOOe/t83lPOgwhvU3AABgD+65AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBYJWioiLdeeeduuKKK+Tj46Pp06d7uyQAHka4AWCVyspKXXTRRUpLS1OPHj28XQ4ALyDcAGhRvvvuO4WHh+tvf/ubq2379u3y8/PTunXr1KVLFz3//PMaM2aMgoODvVgpAG9xersAAGiIiy66SEuXLtXw4cOVmJioK6+8UnfddZcmT56sxMREb5cHoBkg3ABocYYMGaKJEydq9OjRuu666xQQEKAnnnjC22UBaCa4LAWgRXrmmWdUVVWlV199VStWrFBAQIC3SwLQTBBuALRIe/bs0cGDB1VTU6Nvv/3W2+UAaEa4LAWgxTlx4oRGjx6t5ORkXXnllZowYYI+++wzhYWFebs0AM0A4QZAi5OWlqaysjLNmzdPbdu21fvvv68JEybo3XfflSTt3LlTknT06FF999132rlzp/z8/HTVVVd5sWoAnuIwxhhvFwEA9bVx40YNHjxYGzZsUP/+/SVJBQUFuuaaa5Senq777rtPDoej1n7R0dHat2+fh6sF4A2EGwAAYBVuKAYAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKv8H0d/VzxlRfSYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x1,x2, color=['r','g','g','g'])\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('OR problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_or = np.array([-0.5,1,1])\n",
    "step_activation(X_or.dot(w_or))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your weights are correct, the following should output true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.all(step_activation(X_and.dot(w_and)) == Y_and.ravel()))\n",
    "print(np.all(step_activation(X_or.dot(w_or)) == Y_or.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we are already taking advantage of **matrix calculus**: by multiplying above the input matrix with the weight vector we can simultaneously obtain the perceptron's outputs for all patterns. Then we just need to compare whether those outputs are actually the desired ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us code now **Rosenblatt's perceptron**, so that it learns automatically **w_and** and **w_or** for us, as they are both **linearly separable** problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Implement Rosenblatt's perceptron in a function called **perceptron_learn**. The inputs should be the X and Y matrices for the problem to be solved, and the output should be the **w** vector comprising both the bias and the actual weights.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Rosenblatt's algorithm operates in an **online** way, so you cannot take advantage of matrix calculus, as the weight vector **w** may change with every single pattern.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "For comparison purposes, initialize **w = 0**. The function **zeros** in numpy does exactly this.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "\n",
    "def perceptron_learn(X, Y, f):\n",
    "    w = np.zeros(X[0].size)\n",
    "    cont = 0\n",
    "    error = True\n",
    "    while(error == True):\n",
    "        error = False\n",
    "        for i in range(len(X)):\n",
    "            o = f(w.dot(X[i]))\n",
    "            if (o != Y[i][0]):\n",
    "                w = w + (Y[i][0] - o) * X[i]\n",
    "                error = True\n",
    "        cont+=1\n",
    "        if (cont == 1000):\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "\n",
    "    if (cont < 1000):\n",
    "        print (str(cont) + \" epochs needed for convergence.\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Test your implementation with the AND and OR problems. How many **epochs** are needed for convergence? What values do you get for **w_and** and **w_or**?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 epochs needed for convergence.\n",
      "[-3.  2.  1.]\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "perceptron_and = perceptron_learn(X_and, Y_and, step_activation)\n",
    "print(perceptron_and)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epochs needed for convergence.\n",
      "[-1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "perceptron_or = perceptron_learn(X_or, Y_or, step_activation)\n",
    "print(perceptron_or)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Verify that these new values for **w_and** and **w_or** do solve the respective problems. What happens if you initialize weights differently in **perceptron_learn**?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Although Rosenblatt's algorithm states that all weights should be initialized to 0, you can initialize them randomly and convergence is still guaranteed.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.all(step_activation(X_and.dot(perceptron_and)) == Y_and.ravel()))\n",
    "print(np.all(step_activation(X_or.dot(perceptron_or)) == Y_or.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "\n",
    "#Here we will run the same algorithm but with different initial weights\n",
    "\n",
    "w_ = np.array([[1,1,1],[1,0,-1],[-3,3,3]])\n",
    "\n",
    "def perceptron_learn_weights(X, Y, f, w):\n",
    "    cont = 0\n",
    "    error = True\n",
    "    while(error == True):\n",
    "        error = False\n",
    "        for i in range(len(X)):\n",
    "            o = f(w.dot(X[i]))\n",
    "            if (o != Y[i][0]):\n",
    "                w = w + (Y[i][0] - o) * X[i]\n",
    "                error = True\n",
    "        cont+=1\n",
    "        if (cont == 1000):\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "\n",
    "    if (cont < 1000):\n",
    "        print (str(cont) + \" epochs needed for convergence.\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 epochs needed for convergence.\n",
      "[-3  2  1]\n",
      "7 epochs needed for convergence.\n",
      "[-3  2  1]\n",
      "2 epochs needed for convergence.\n",
      "[-4  3  2]\n"
     ]
    }
   ],
   "source": [
    "for i in w_ :\n",
    "    aux = perceptron_learn_weights(X_and,Y_and,step_activation, i)\n",
    "    print(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### For the first 2 initial vectors that we tried, we obtained the same final w that with the previous implementation. For the other vector we obtained a different solution. Does it solve the AND problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.all(step_activation(X_and.dot(np.array([-4,3,2]))) == Y_and.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Notice that with this initial weights the algorithm needed less epochs to converge, that is because the initial vector is \"closer\" to a w that solves the problem than the other initial weights that we tried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epochs needed for convergence.\n",
      "[-1  1  1]\n",
      "6 epochs needed for convergence.\n",
      "[-1  1  1]\n",
      "1 epochs needed for convergence.\n",
      "[-3  3  3]\n"
     ]
    }
   ],
   "source": [
    "for i in w_ :\n",
    "    aux = perceptron_learn_weights(X_or,Y_or,step_activation, i)\n",
    "    print(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Here we are in a very similar situation to the previous case: in two instances we obtained the same w vector as initiating with the vector 0, and we obtain a different solution that, as we can see, is also valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.all(step_activation(X_or.dot(np.array([-3,3,3]))) == Y_or.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare our implementation with that of *scikit-learn*. The class which implements a perceptron is **Perceptron**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "Perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make things comparable, we need no regularization and not shuffling the patterns in each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "percep = Perceptron(alpha = 0.0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Train the scikit-learn perceptron for the AND and OR problems. Do you obtain the same values for **w_and** and **w_or**? Why/why not?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Make sure that the parameter **n_iter** is at least as large as the number of epochs you obtained before.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Since *scikit-learn* splits weights (**coef_**) from biases (**intercept_**), we do not need to prepend anymore a 1 to the patterns. Be careful when feeding them to the **fit** method. Also, take this into account when checking the perceptron's output and comparing it to the one obtained with your method **perceptron_learn**.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_and = [[-4.  3.  2.]]\n",
      "Number of epochs needed to reach w_and : 10\n",
      "w_or = [[-1.  2.  2.]]\n",
      "Number of epochs needed to reach w_or : 10\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "\n",
    "# We write \"fit_intercept = False\" so that when feeding the data to the perceptron through the \"fit\" method, it won't compute biases (b = [0,0,0]).\n",
    "# We do this so we can better compare the output to the one obtained with our implementation.\n",
    "\n",
    "percep_and = Perceptron(alpha = 0.0, shuffle=False, fit_intercept = False)\n",
    "percep_or = Perceptron(alpha = 0.0, shuffle=False, fit_intercept = False)\n",
    "\n",
    "percep_and.fit(X_and, Y_and.ravel())\n",
    "\n",
    "percep_or.fit(X_or, Y_or.ravel())\n",
    "\n",
    "print(\"w_and = \" + str(percep_and.coef_))\n",
    "print(\"Number of epochs needed to reach w_and : \" + str(percep_and.n_iter_))\n",
    "\n",
    "print(\"w_or = \" + str(percep_or.coef_))\n",
    "print(\"Number of epochs needed to reach w_or : \" + str(percep_and.n_iter_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know from the theory, Rosenblatt's perceptrons can only solve **linearly separable** problems. The AND and OR problems fall into this category, but the XOR problem does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Define the XOR problem in two matrices **X_xor**, **Y_xor** as we did above for the AND and OR problems.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "X_xor = X_and.copy()\n",
    "Y_xor = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Verify that **perceptron_learn** does not converge when given the XOR problem.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### When we implemented the \"perceptron_learn\" method, we did it in such a way that if after 1000 epochs we haven't obtained a w vector that solves the problem we will assume the method does not converge and stop iterating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Introduce some control to exit the function after a maximum number of epochs has been reached. Otherwise, execution will go on forever and can stall your PC.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max iter reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "perceptron_learn(X_xor,Y_xor,step_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Verify that scikit-learn's **Perceptron** does not converge either for the XOR problem.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "percep_xor = Perceptron(alpha = 0.0, shuffle=False)\n",
    "percep_xor.fit(X_xor, Y_xor.ravel())\n",
    "percep_xor.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### When using the scikit-learn implementation, we obtain w = (0,0,0), wich obviously doesn't solve the XOR problem,so we can say this method does not converge either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the limitations perceptrons have, **multilayer perceptrons (MLPs)** are usually the choice when dealing with general problems. Let us use for now the following class for an MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that an MLP is initialized with a list specifying the sizes of the different layers. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sizes = [2, 3, 1]\n",
    "net = MLP(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an MLP with 2 input neurons, 3 hidden neurons and 1 output neuron. <u>Note also the convention of the weights: they are created in such a way that *weights[i][j][k]* denotes the weight connecting neuron k of the i-th layer to neuron j of the (i+1)-th layer</u> (assuming that input layer is layer 0, first hidden layer is layer 1, and so on). <u>The same logic applies for biases, so that *biases[i][j]* is the bias of neuron j of the (i+1)-th layer</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 3\n",
      "Sizes of layers: [2, 3, 1]\n",
      "Biases of hidden layer: [[0.3796777 ]\n",
      " [0.43891417]\n",
      " [1.34517718]]\n",
      "Biases of output layer: [[-0.17962782]]\n",
      "Weights between input and hidden layer: [[-0.76818408 -0.52300753]\n",
      " [-1.97272886 -1.33290922]\n",
      " [ 0.3504558  -1.44088489]]\n",
      "Weights between hidden and output layer: [[ 1.11513867 -1.29830328  0.64512092]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers: \" + str(net.num_layers))\n",
    "print(\"Sizes of layers: \" + str(net.sizes))\n",
    "print(\"Biases of hidden layer: \" + str(net.biases[0]))\n",
    "print(\"Biases of output layer: \" + str(net.biases[1]))\n",
    "print(\"Weights between input and hidden layer: \" + str(net.weights[0]))\n",
    "print(\"Weights between hidden and output layer: \" + str(net.weights[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume for simplicity that all **activation functions** in our MLPs are going to be the *step_activation* defined above. Note that its implementation is vectorized, so that it works both for scalars and numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily program the **forward phase** of the **back-propagation** algorithm, that is, to input a pattern to the network and compute the network's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Implement the function **forward_phase(mlp, x)** that, given an MLP and an input vector **x**, computes the MLP's outputs when **x** is fed.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Take advantage of matrix calculus. Make sure to reshape the input vector to column form, so that the matrix-vector products do not raise errors.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "\n",
    "def forward_phase(mlp,x):\n",
    "    x = x.reshape(-1,1)\n",
    "    step1 = np.dot(mlp.weights[0],x) + mlp.biases[0]\n",
    "    step2 = np.dot(mlp.weights[1],step_activation(step1)) + mlp.biases[1]\n",
    "    return step_activation(step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since weights in the MLP class are initialized randomly, it is very unlikely that these initial weights actually solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Check whether the MLP created above does solve XOR or not.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Again, the MLP class splits weights from biases, so you should not feed to the networks the ones prepended to the patterns.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Because of matrix calculus, the return of **forward_phase** will be in matrix form, when it is actually a scalar since there is only a single output neuron. You may need to flatten return values to compare them to the actual outputs.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[[0]]\n",
      "[[1]]\n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "print(forward_phase(net, np.array([0,0])))\n",
    "print(forward_phase(net, np.array([0,1])))\n",
    "print(forward_phase(net, np.array([1,0])))\n",
    "print(forward_phase(net, np.array([1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Build an MLP that actually solves XOR.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "You know from the theory that it suffices with a hidden layer of just 2 neurons. Because we have not coded any learning algorithm (we would need to program the whole back-propagation algorithm for that), you will have to set directly its weights and biases so that it does the job.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "\n",
    "###########################\n",
    "# ESTO FALTA POR HACER\n",
    "###########################\n",
    "\n",
    "sizes = [2, 2, 1]\n",
    "net_xor = MLP(sizes)\n",
    "\n",
    "net_xor.weights[0] = np.array([[1,-1],[-1,1]])\n",
    "net_xor.weights[1] = np.array([[-1,1]])\n",
    "net_xor.biases[0] = np.array([[-1],[-1]])\n",
    "net_xor.biases[1] = np.array([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[[1]]\n",
      "[[1]]\n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "print(forward_phase(net_xor, np.array([0,0])))\n",
    "print(forward_phase(net_xor, np.array([0,1])))\n",
    "print(forward_phase(net_xor, np.array([1,0])))\n",
    "print(forward_phase(net_xor, np.array([1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding oneself the back-propagation algorithm is tedious and prone to errors (especially the **backward phase**), so it is only useful as an academic programming exercise. In practice, one resorts to implementations already available. *Scikit-learn* has two classes for MLPs, the **MLPClassifier** and the **MLPRegressor**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier()\n",
      "MLPRegressor()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "print(MLPClassifier())\n",
    "print(MLPRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only differences between the two are the **loss function** (**cross-entropy** for classification, **MSE** for regression) and the activation function of the output layer (**sigmoid** for classification, **identity** for regression). As you can see, the parameters used in construction are exactly the same ones, as well as the default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Discuss which of the above parameters you can identify with those seen in the theory slides and which you cannot.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We've seen both parameters in the theory slides.\n",
    "\n",
    "##### Loss function: Mean squared error is the loss funtion usually used for regression problems, not only for the multi-layer perceptron. Cross-entropy refers to the \"log loss\" function, which penalizes incorrect classifications with increasing severity as the prediction diverges from the true class label.\n",
    "##### Activation function: Determines the output of each neuron of the output layer. As we saw on class, using identity as the activation function in the output layer is what allows regression. Sigmoid is usually used for classification because it's covenient for calcullating the gradient as we studied in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Take some classification dataset used in the SVM assignments and fit an *MLPClassifier* by modifying the parameters you deem appropriate. Report the best network configuration you can find. Can you beat the best SVM you obtained for that problem?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### We will use the \"adult1\" dataset used in the SVM assignment. In a usual situation we wouln't usually do cross-validation for a neural network model due to it's computational cost. But for the purpose of this assignment (and because we have a small dataset) we will use \"GridSearchCV\" for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file, load_svmlight_files\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_svmlight_files((\"./data/adult1.svm\", \"./data/adult1_test.svm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('standard_scaler', StandardScaler(with_mean=False)),\n",
    "    ('mlpc', MLPClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Because of the computational cost that we already mentioned, we aren't able to use a lot of parameters in GridSearchCV, so we picked a few that we considered significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;standard_scaler&#x27;,\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       (&#x27;mlpc&#x27;, MLPClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;mlpc__activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;, &#x27;logistic&#x27;],\n",
       "                         &#x27;mlpc__hidden_layer_sizes&#x27;: [(100,), (100, 50)],\n",
       "                         &#x27;mlpc__learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;mlpc__learning_rate_init&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;mlpc__max_iter&#x27;: [200, 300, 400],\n",
       "                         &#x27;mlpc__solver&#x27;: [&#x27;adam&#x27;, &#x27;sgd&#x27;, &#x27;lbfgs&#x27;],\n",
       "                         &#x27;mlpc__tol&#x27;: [0.0001, 0.001, 0.01]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;standard_scaler&#x27;,\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       (&#x27;mlpc&#x27;, MLPClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;mlpc__activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;, &#x27;logistic&#x27;],\n",
       "                         &#x27;mlpc__hidden_layer_sizes&#x27;: [(100,), (100, 50)],\n",
       "                         &#x27;mlpc__learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;mlpc__learning_rate_init&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;mlpc__max_iter&#x27;: [200, 300, 400],\n",
       "                         &#x27;mlpc__solver&#x27;: [&#x27;adam&#x27;, &#x27;sgd&#x27;, &#x27;lbfgs&#x27;],\n",
       "                         &#x27;mlpc__tol&#x27;: [0.0001, 0.001, 0.01]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standard_scaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;mlpc&#x27;, MLPClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('standard_scaler',\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       ('mlpc', MLPClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'mlpc__activation': ['relu', 'tanh', 'logistic'],\n",
       "                         'mlpc__hidden_layer_sizes': [(100,), (100, 50)],\n",
       "                         'mlpc__learning_rate': ['constant', 'adaptive'],\n",
       "                         'mlpc__learning_rate_init': [0.001, 0.01, 0.1],\n",
       "                         'mlpc__max_iter': [200, 300, 400],\n",
       "                         'mlpc__solver': ['adam', 'sgd', 'lbfgs'],\n",
       "                         'mlpc__tol': [0.0001, 0.001, 0.01]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameter_grid = {\n",
    "    'mlpc__hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'mlpc__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'mlpc__solver': ['adam', 'sgd', 'lbfgs'],\n",
    "    'mlpc__learning_rate': ['constant', 'adaptive'],\n",
    "    'mlpc__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'mlpc__max_iter': [200, 300, 400],\n",
    "    'mlpc__tol': [1e-4, 1e-3, 1e-2],\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(pl, parameter_grid,  cv=5, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standard_scaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;mlpc&#x27;, MLPClassifier(max_iter=300, solver=&#x27;sgd&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standard_scaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;mlpc&#x27;, MLPClassifier(max_iter=300, solver=&#x27;sgd&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=300, solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standard_scaler', StandardScaler(with_mean=False)),\n",
       "                ('mlpc', MLPClassifier(max_iter=300, solver='sgd'))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8332794934746092\n"
     ]
    }
   ],
   "source": [
    "my_best_estimator = gs.best_estimator_\n",
    "y_pred = my_best_estimator.predict(X_test)\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### We obtained a lower accuracy than with the SVC model (0.892). This could be caused by different reasons: \n",
    "##### 1. The fact that in the dataset we chose, there are few training data (1605) as opposed to test data (30956), but this issue is shared by both models.\n",
    "##### 2. We may need to find a better hyperparameter configuration.\n",
    "##### 3. The SVC model may just be more apropiate for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Repeat with some regression dataset and an *MLPRegressor*. Are you able to beat the SVR?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pl2 = Pipeline([\n",
    "    ('standard_scaler', StandardScaler(with_mean=False)),\n",
    "    ('mlpr', MLPRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "sss = ShuffleSplit(n_splits=1,test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in sss.split(X,y):\n",
    "    \n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_test, y_test = X[test_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "201 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "201 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 753, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 496, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\migue\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [ 6.66017959e-001  6.51820796e-001  5.57700537e-001  9.83904209e-002\n",
      "  1.78249187e-001  1.29434529e-001  5.81606885e-001  5.72597438e-001\n",
      "  5.86022609e-001  6.15831725e-001  6.22709039e-001  5.26246167e-001\n",
      "  3.00347918e-001  1.62932435e-001 -1.90874220e-001  6.19451631e-001\n",
      "  6.15216068e-001  6.28915784e-001  6.64160528e-001  6.53193933e-001\n",
      "  5.73244104e-001  1.10641227e-001  2.96360041e-001 -2.94809519e-002\n",
      "  6.37303152e-001  6.51961585e-001  6.27815234e-001  6.12781412e-001\n",
      "  5.92870031e-001  6.05346783e-001 -8.62076388e+015 -4.46652809e-004\n",
      " -3.32004286e-004  5.80234709e-001  5.69731803e-001  5.68440644e-001\n",
      "  6.43819688e-001  6.20476386e-001  4.61305960e-001 -1.83768800e+147\n",
      " -8.11541356e+073 -1.70440224e+010  6.19328163e-001  6.09859175e-001\n",
      "  6.16255213e-001  6.07889992e-001  6.21188453e-001  4.37219485e-001\n",
      " -1.47762243e+054 -3.25652639e+083 -1.56338699e-003  6.35322014e-001\n",
      "  6.35863365e-001  6.55115796e-001  5.67714099e-001  3.93218751e-001\n",
      "  5.18509793e-001 -7.68458072e+038 -2.64584575e+046 -2.15069787e+075\n",
      "  5.57604373e-001  5.88722364e-001  5.68414801e-001  4.27760923e-001\n",
      "  5.82111063e-001  5.32664794e-001 -2.07849212e+025 -1.30386556e+056\n",
      " -3.17090743e+087  6.25458481e-001  6.06842376e-001  6.01144878e-001\n",
      "  5.76797493e-001  5.66840756e-001  6.07572459e-001 -2.48443029e+062\n",
      " -3.37562314e+003 -6.10917117e+129  6.27368346e-001  6.36057517e-001\n",
      "  6.48239888e-001  6.70042546e-001  6.59389005e-001  5.41980362e-001\n",
      "  2.29090375e-001  4.49937317e-001  3.23894872e-001  5.73648890e-001\n",
      "  5.83528112e-001  5.67465766e-001  6.16518998e-001  6.51333162e-001\n",
      "  5.79860417e-001  2.27533526e-001  4.48858453e-001  3.30554177e-001\n",
      "  6.03593734e-001  6.13228478e-001  6.12881498e-001  6.55403760e-001\n",
      "  6.39654469e-001  5.85002062e-001  4.52981209e-001  3.34782362e-001\n",
      "  4.35776213e-001  6.36378808e-001  6.32016919e-001  6.24752023e-001\n",
      "  5.60323982e-001  2.63725019e-001  5.54264588e-001 -1.57372266e-004\n",
      " -1.01307704e+038 -1.51652087e-004  5.75148246e-001  5.77087618e-001\n",
      "  5.90024205e-001  5.86498968e-001  5.75617560e-001  3.15800108e-001\n",
      " -1.30844345e-004 -2.77153523e-004 -1.04867051e-003  6.35278877e-001\n",
      "  6.22276529e-001  6.10969341e-001  6.04137044e-001  5.94000973e-001\n",
      "  4.89984613e-001 -4.83503196e+039 -4.57187430e-004 -3.96166792e+085\n",
      "  6.18432281e-001  6.49597077e-001  6.32938890e-001  2.97030735e-001\n",
      "  5.23203084e-001  5.70550841e-001 -2.14222024e-004 -7.56739630e-002\n",
      " -1.70980939e-004  5.70275461e-001  5.89399372e-001  5.75382731e-001\n",
      "  5.87700778e-001  5.03042285e-001  5.73993269e-001 -1.34177764e-003\n",
      " -6.57731862e+032              nan  6.25904214e-001  6.00155900e-001\n",
      "  6.10572151e-001  4.43861795e-001  4.25479627e-001  5.12078023e-001\n",
      " -1.70761311e-004 -6.06170219e+025              nan  6.42466735e-001\n",
      "  6.32324139e-001  6.48805917e-001  6.74957854e-001  6.45444812e-001\n",
      "  4.70726079e-001              nan -2.20069329e+132              nan\n",
      "  5.78707782e-001  5.85619866e-001  5.82094498e-001  5.85086132e-001\n",
      "  6.37599739e-001  5.50284081e-001 -3.79502445e+137  4.65271583e-002\n",
      "              nan  6.20393360e-001  6.24619295e-001  6.08024429e-001\n",
      "  6.52834560e-001  6.01023592e-001  6.16413419e-001              nan\n",
      "              nan              nan  6.36804291e-001  6.46689052e-001\n",
      "  6.52191844e-001  6.46230774e-001  5.71865327e-001  5.92519980e-001\n",
      "              nan              nan              nan  5.50500373e-001\n",
      "  5.72199177e-001  5.85502797e-001  6.25051924e-001  6.38590254e-001\n",
      "  6.43880796e-001              nan              nan              nan\n",
      "  5.95522629e-001  6.17266659e-001  6.14978143e-001  6.26656076e-001\n",
      "  6.59935023e-001  6.40200611e-001              nan              nan\n",
      "              nan  6.41888810e-001  6.53933289e-001  6.49913643e-001\n",
      "  3.51244842e-001  5.47665214e-001  4.70713594e-001              nan\n",
      "              nan              nan  5.76371034e-001  5.95755740e-001\n",
      "  5.99083683e-001  5.81161345e-001  3.94787593e-001  4.56071030e-001\n",
      "              nan              nan              nan  6.09140149e-001\n",
      "  6.17618880e-001  6.17971614e-001  4.86540018e-001  4.92339572e-001\n",
      "  3.57684736e-001              nan              nan              nan\n",
      "  6.42300896e-001  6.57475554e-001  6.36562723e-001  6.51441170e-001\n",
      "  6.22222789e-001  5.21422816e-001 -4.21238645e+123              nan\n",
      "              nan  5.91571610e-001  5.90529870e-001  5.90551896e-001\n",
      "  6.18229292e-001  6.46947287e-001  6.04511075e-001              nan\n",
      " -1.40678301e+118              nan  6.38030444e-001  6.28653579e-001\n",
      "  5.96093411e-001  6.23357522e-001  6.61517043e-001  5.77311497e-001\n",
      "              nan              nan              nan  6.31525429e-001\n",
      "  6.48574874e-001  6.38898374e-001  5.86769279e-001  6.19102935e-001\n",
      "  6.50454659e-001              nan              nan              nan\n",
      "  5.85239525e-001  5.96679160e-001  5.78459315e-001  5.96094105e-001\n",
      "  6.01097235e-001  6.14853848e-001              nan              nan\n",
      "              nan  6.34776478e-001  6.22913954e-001  6.26520902e-001\n",
      "  6.69138684e-001  6.32906085e-001  4.59847454e-001              nan\n",
      "              nan              nan  6.38469988e-001  6.28519793e-001\n",
      "  6.38305466e-001  4.64678892e-001  3.87088087e-001  5.96821653e-001\n",
      "              nan              nan              nan  5.89502287e-001\n",
      "  5.82934059e-001  5.77092584e-001  3.59885049e-001  4.02927971e-001\n",
      "  3.31176417e-001              nan              nan              nan\n",
      "  6.12837634e-001  6.24693322e-001  5.99305097e-001  5.85640706e-001\n",
      "  3.70842924e-001  5.61355459e-001              nan              nan\n",
      "              nan  6.38007319e-001  6.41523169e-001  6.34653057e-001]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;standard_scaler&#x27;,\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       (&#x27;mlpr&#x27;, MLPRegressor())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;mlpr__hidden_layer_sizes&#x27;: [(100,), (100, 50)],\n",
       "                         &#x27;mlpr__learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;mlpr__learning_rate_init&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;mlpr__max_iter&#x27;: [200, 300, 400],\n",
       "                         &#x27;mlpr__solver&#x27;: [&#x27;adam&#x27;, &#x27;sgd&#x27;, &#x27;lbfgs&#x27;],\n",
       "                         &#x27;mlpr__tol&#x27;: [0.0001, 0.001, 0.01]},\n",
       "             scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;standard_scaler&#x27;,\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       (&#x27;mlpr&#x27;, MLPRegressor())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;mlpr__hidden_layer_sizes&#x27;: [(100,), (100, 50)],\n",
       "                         &#x27;mlpr__learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;mlpr__learning_rate_init&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;mlpr__max_iter&#x27;: [200, 300, 400],\n",
       "                         &#x27;mlpr__solver&#x27;: [&#x27;adam&#x27;, &#x27;sgd&#x27;, &#x27;lbfgs&#x27;],\n",
       "                         &#x27;mlpr__tol&#x27;: [0.0001, 0.001, 0.01]},\n",
       "             scoring=&#x27;r2&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standard_scaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;mlpr&#x27;, MLPRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('standard_scaler',\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       ('mlpr', MLPRegressor())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'mlpr__hidden_layer_sizes': [(100,), (100, 50)],\n",
       "                         'mlpr__learning_rate': ['constant', 'adaptive'],\n",
       "                         'mlpr__learning_rate_init': [0.001, 0.01, 0.1],\n",
       "                         'mlpr__max_iter': [200, 300, 400],\n",
       "                         'mlpr__solver': ['adam', 'sgd', 'lbfgs'],\n",
       "                         'mlpr__tol': [0.0001, 0.001, 0.01]},\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_grid2 = {\n",
    "    'mlpr__hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'mlpr__solver': ['adam', 'sgd', 'lbfgs'],\n",
    "    'mlpr__learning_rate': ['constant', 'adaptive'],\n",
    "    'mlpr__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'mlpr__max_iter': [200, 300, 400],\n",
    "    'mlpr__tol': [1e-4, 1e-3, 1e-2],\n",
    "}\n",
    "\n",
    "gs2 = GridSearchCV(pl2, parameter_grid2,  cv=5, scoring='r2', n_jobs=-1)\n",
    "gs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_best_estimator2 = gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(steps=[('standard_scaler', StandardScaler(with_mean=False)),\n",
       "                ('mlpr', MLPRegressor(hidden_layer_sizes=(100, 50)))])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_best_estimator2.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_escaler = StandardScaler(with_mean = False)\n",
    "\n",
    "my_escaler.fit(X_train)\n",
    "X_test_norm = my_escaler.transform(X_test)\n",
    "y_pred = my_best_estimator2.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0315232112295734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Beware of normalizing your data before feeding them to an MLP. It is advised to use a pipeline with a *StandardScaler*.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "Once in a pipeline, you can use grid search to try different choices for the MLP parameters.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>\n",
    "                          THIS IS THE END OF THE ASSIGNMENT<br>\n",
    "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
